---
layout: post
title:  "Action the Sounds"
info: "A deep network for the  interactions between sound and robotic action."
tech : "PyTorch"
type: Research leaded by CMU
---
<h2><center>A deep network for the  interactions between sound and robotic action</center></h2>

Click here to see the [code](https://github.com/mtzhang1999/Project-Matching-based-on-Audio-and-Image-Sequences).

Based on this research: [Swoosh! Rattle! Thump! - Actions that Sound](https://dhiraj100892.github.io/swoosh/)

## Backgrounds

In recent years, computer vision and hearing technologies have achieved great success in many tasks, such as image classification, object detection, speech recognition, etc. It is often easier to model a single individual, but it is more difficult to model the audiovisual information generated by the interaction between objects.

We provide the following scenario. The object is placed in a metal tray, the tray is tilted in any direction, and the object collides with the tray wall and produces sounds. A microphone is placed in the center of each of the four sides of the tray wall to record audio, and the camera captures RGB images vertically downwards above the tray. 

Given collision paragraphs of different objects and different motion states (each paragraph contains audio with a duration of 4 seconds centered on the collision moment, and the corresponding RGB image sequence). Both audio and image sequences contain the characteristics of objects and their motion states. Therefore, the matching of audio and image sequences can be achieved through reasonable modeling and learning of the above features.

## Intro to our dataset

The dataset contains impact audio of 10 different objects, and each audio segment is intercepted within 2 seconds before and after the time of the impact. The audio of the four channels comes from the microphones at the center of the four sides of the tray. The audio sampling frequency of each channel is $$44.1kHz$$.

The dataset also provides a sequence of RGB images corresponding to the audio, and the size of each image is $$480\times 640$$. In order to easily obtain the information of the object position, we provide the object mask sequence corresponding to the RGB image sequence.

The mask sequence is obtained by performing a simple background subtraction on the RGB sequence. In order to exclude the influence of the background from the tray on the background subtraction, the RGB image is first cropped, with $$100$$ pixels on the left and right, and $$20$$ pixels on the top and bottom. The resulting mask size is $$440\times 440$$.

Only the THU lab members have the access to the datasets for our projects. You may learn more details from the [released dataset](https://github.com/Dhiraj100892/swoosh) owned by CMU.

## Task Description

### Task 1

The physical properties of different objects vary, so the impact audio also has different characteristics.

Task 1 uses only the four-channel audio information of the object impact to classify the types of objects, and estimate the number of the classified objects for each audio file.

