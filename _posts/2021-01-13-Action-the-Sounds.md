---
layout: post
title:  "Action the Sounds"
info: "A deep network for the  interactions between sound and robotic action."
tech : "PyTorch"
type: Research leaded by CMU
---
<h2><center>A deep network for the  interactions between sound and robotic action</center></h2>

Click here to see the [code](https://github.com/mtzhang1999/Project-Matching-based-on-Audio-and-Image-Sequences).

Based on this research: [Swoosh! Rattle! Thump! - Actions that Sound](https://dhiraj100892.github.io/swoosh/)

## Backgrounds

In recent years, computer vision and hearing technologies have achieved great success in many tasks, such as image classification, object detection, speech recognition, etc. It is often easier to model a single individual, but it is more difficult to model the audiovisual information generated by the interaction between objects.

We provide the following scenario. The object is placed in a metal tray, the tray is tilted in any direction, and the object collides with the tray wall and produces sounds. A microphone is placed in the center of each of the four sides of the tray wall to record audio, and the camera captures RGB images vertically downwards above the tray. Given collision paragraphs of different objects and different motion states (each paragraph contains audio with a duration of 4 seconds centered on the collision moment, and the corresponding RGB image sequence). Both audio and image sequences contain the characteristics of objects and their motion states. Therefore, the matching of audio and image sequences can be achieved through reasonable modeling and learning of the above features.